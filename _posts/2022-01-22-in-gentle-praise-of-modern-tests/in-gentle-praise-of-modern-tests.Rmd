---
title: "In gentle praise of classifier tests"
description: |
  Two-sample statistical tests based on modern high-capacity
  classifiers are powerful and still, underappreciated.
author:
  - name: Vathy M. Kamulete
    url: https://www.vathymut.org/
    orcid_id: 0000-0002-4451-3743
base_url: https://www.vathymut.org/
date: 2022-01-22
output:
  distill::distill_article:
    self_contained: false
draft: false
bibliography: reference.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

The late great statistician, [Sir David Cox](https://en.wikipedia.org/wiki/David_Cox_(statistician)),
has a talk titled [In gentle praise of significance tests](https://youtu.be/txLj_P9UlCQ).
It is, of course, only *gentle praise* because inevitably, there are some
issues. I too have a bone to pick. But
we will get to the beef much later. First, the *praise*.

Remember the old two-sample tests? They go by different names: test of equal
distribution, of homogeneity, of goodness-of-fit, etc. The gist of it is you
have two samples. And so, you wonder, are they similar enough? Rings a bell,
right? The [Kolmogorovâ€“Smirnov
test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) is a 
classic for this sort of thing. But the
<abbr title="Kolmogorovâ€“Smirnov">K-S</abbr> test is old news, well before the
advent of *big* and *high dimensional* data ðŸ˜±.

Since then, the field is churning out powerful two-sample tests for the Big
Data era. Every year, exciting works are published on the subject. I
recently messed about with the Ball divergence approach [@pan2018ball].
You, no doubt, chose your own adventure. There has been tremendous
progress. Let Uncle Larry
[explain](https://normaldeviate.wordpress.com/2012/07/14/modern-two-sample-tests/)^[Click
on that link. I won't be offended; I will in fact, happily, wait.].
And now you are, more or less, caught up with the state-of-the-art in
two-sample tests. The End.

<aside>
That is, err <a href="https://en.wikipedia.org/wiki/Larry_A._Wasserman">Larry A.
Wasserman</a>.
</aside>

OK, not so fast. You see, Uncle Larry left out
something important. He touches on 3 ideas: kernel, energy, and cross-match
tests. Trust me when I say, he wants you to know about the classifier tests too
[@kim2021classification]. Here is how they work. You assign your samples to
different classes (you give them labels). Sample 1 is the positive class,
and Sample 2, the negative one or vice versa. Then, you fit your
favorite classifier to see if it can reliably predict the label. If
it can with high accuracy, it means the two samples are probably different. If
it cannot, the two samples are similar enough. It seems obvious in hindsight,
does it not? It is not even *deceptively* simple: it is *actually* simple.

<aside>
Details, casually swept under the rug, include deriving $p-$value from the
classifier performance. The short answer is permutation is your friend.
</aside>

The thing is most people do not seem to know about two-sample classifier tests.
In comparison, kernel tests seem ubiquitous (if you are looking at academic
journals, not if you're hanging out on Twitter). Even the energy tests, one of the
innovation that Uncle Larry discusses, are in a fundamental sense equivalent to
the kernel tests [@sejdinovic2012hypothesis; @shen2020exact]. It is kernels all
the way down. On the theoretical front, the mathematical maturity
to wield this awesome kernel power may require arcane incantations to
<abbr title="Reproducing kernel Hilbert space">RKHS</abbr> voodoo magic.
Still, it begs the question. How do classifier tests stack up against kernel
tests?

No spoiler from me. Let @lopez2017revisiting do it:

<blockquote>
Our take-home message is that modern binary classifiers can be easily turned
into powerful two-sample tests. We have shown that these classifier two-sample
tests set a new state-of-the-art in performance, and enjoy unique attractive
properties: they are easy to implement, learn a representation of the data
on the fly, have simple asymptotic distributions, and allow different ways to
interpret how the two samples under study differ.
</blockquote>

They do not often tell you this. You can in fact
think of [a kernel test as a classifier test](https://openreview.net/forum?id=SJkXfE5xx&noteId=ry-le414x).
And you know what, data scientists are really good at supervised
classification. Talk $p-$values, eyes glaze
over and no one is listening. Say 50% accuracy instead, suddenly everyone
understands and is nodding vigorously. No RKHS magic needed -- this is yuge. 

<iframe src="https://giphy.com/embed/NM2SEXpp58mwU" width="480" height="265" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p><a href="https://giphy.com/gifs/mic-snl-bernie-sanders-feel-the-bern-NM2SEXpp58mwU">From GIPHY</a></p>

Here is @cai2020two, more recently, to hammer the point home:

<blockquote>
We propose a test for two-sample problem based on estimates of classification
probabilities obtained from a consistent classification algorithm. [...] Our
test is more powerful and efficient than many other tests.
</blockquote>

Practicing data scientists cannot afford to ignore two-sample classifier tests.
They are powerful, easy to implement and easy to explain, the elusive
trifecta^[Think the son, the father and the holy spirit, the divine
trinity.]. Plenty of works advocate for the
classifier tests [@friedman2004multivariate; 
@vayatis2009auc; @liu2018classifier; @hediger2019use]. Still, 
they remain for the most part underappreciated. No one
(anyone you know?) brags about them, the way for example that they would about
the newest kernel test on the block. I suspect it is because the theory seems
*boring* in comparison. In practice, these guys pack a punch. This is worth
praising. The two-sample classifier tests need more love. They deserve it.

