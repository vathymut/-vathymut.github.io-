---
title: "Are you OK? Test for harmful (adverse) shift"
description: |
  Beyond statistical tests of equal distribution and of mean differences for
  dataset shift.
author:
  - name: Vathy M. Kamulete
    url: https://www.vathymut.org/
    orcid_id: 0000-0002-4451-3743
base_url: https://www.vathymut.org/
date: 2023-01-03
output:
  distill::distill_article:
    self_contained: false
draft: false
bibliography: reference.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Sometimes the pertinent question is, **are we worse off?**
[Annie](https://youtu.be/CDl9ZMfj6aE), what we really want to know is,
are you OK? We compare the way things were (the happy past) to the way they
are (the tumultuous present). But wait, your statistical mind makes the leap,
is this not just a two-sample comparison (test)? It *is*. But the trouble is
not all statistical tests are well-suited for this task. The main point of
this post is to argue that some widely used statistical tests may come
close but often miss the mark when it comes to answering this question. The
Question &copy; -- that is, once more so you don't forget, are we worse
off? -- is what I mean when I say testing for harmful or adverse shift.

<aside>
The more covert point, the less altruistic one, is to motivate
my own research, of course.
</aside>

But first off, you may wonder, why does it matter? One reason to care in machine
learning is dataset shift. Over time, things change, data drift and predictive
models, evidently built on past data,
suffer as a consequence. Model performance deteriorates, sometimes quite
drastically. Chip Huyen has a great primer on dataset shift
[here](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html)
- it is a *fantastic* resource (go read it!). It turns out, to detect dataset
shift, we often rely on statistical tests. The most common ones are
tests of equal distribution and of mean differences. And I posit, both
let you down in perhaps surprising ways when it comes down to the Question &copy;
~~Which is? You still with me, right?~~

In a previous
[post](https://vathymut.org/posts/2022-01-22-in-gentle-praise-of-modern-tests/),
I praised modern tests of equal distribution. But I did warn you that I had
some beef too. It is time to settle the score. These tests fail us when testing
for adverse shift^[It isn't their fault; it's ours, the users. They simply
do what they were designed to do, no more, no less, as infuriating as that
may be.].They fail
because "not all changes in distribution are a cause for concern - some
changes are benign" [@kamulete2022test]. A simple example can
convince you of this. Suppose your original sample was
contaminated with a few outliers but your new
sample is not. You're clearly better off. Well, tests of equal
distribution would still tell you that the two samples are *different* i.e.
you would reject the null of no difference. This is a false alarm. These
tests don't answer the Question &copy;; they tell us when the two samples
can be said to be drawn from the same distribution, not whether we are worse
off. As we know all too well, *different* does not mean *inferior*. We need
a better test.

<aside>
If you are a R user (get it? never mind), the
<a href="https://CRAN.R-project.org/package=dsos">dsos package</a>
implements the approach in @kamulete2022test.
</aside>

What about the statistical tests for mean differences? @lindon2022rapid 
explains the problem:

<blockquote>
[..] Not all bugs or performance regressions can be captured by
differences in the mean alone [...]. Consider PlayDelay, an important key
indicator of the streaming quality at Netflix, which measures the time taken
for a title to start once the user has hit the play button. It is possible for
the mean [...] to be the same, but for the tails of the
distribution to be heavier [...], resulting in an increased
occurrence of extreme values. Large values of PlayDelay, even if infrequent,
are unacceptable and considered a severe performance regression by our
engineering teams.
</blockquote>

Again, we need a better test. Incidentally, if you were not already convinced
that the Question &copy; is important, I *know* you are now: it is your Netflix
bingeing at stake here. In short, both tests of
equal distribution and of mean differences are lacking when it comes to the
Question &copy;. That's the bad news.

What's the good news? If these widely used statistical
tests are not good enough, what are the alternatives? I referred to two already
[@kamulete2022test; @lindon2022rapid]^[Full disclosure: I am the author of
@kamulete2022test.]. Recent works
[@podkopaev2021tracking; @luo2022online; @vovk2021retrain; @harel2014concept]
also attempt to tackle the Question &copy; more rigorously (precisely)
^[One paradigm that to my mind does not get its due
in ML/AI circles is statistical process control or statistical quality
control [@gandy2017spcadjust; @flores2021statistical]. I am, unhappily I
assure you, complicit in that here. This community also cares about the
the Question &copy;. In fact, some may rightfully say they were pioneers in
this area.]. When discussing my own work and the `dsos` package at
[useR! 2022](https://youtu.be/TALE9JUir8Q?t=26), I chose the title
"Call me when it hurts" because it gets at the core of the
Question &copy;. Data scientists want to know when there *is* a real problem: 
raising too many false alarms is one of the fastest and surest way to lose
credibility.

<aside>
By the way, the Safe, Anytime-Valid Inference
<a href="https://www.stat.cmu.edu/~aramdas/SAVI/savi20.html">(SAVI)</a>
2022 workshop is a great place to start if you're looking
for more references.
</aside>

Let me conclude. If you stuck with me this far, first I am sorry, not
sorry ðŸ˜‰, for abusing the Question &copy; to mean testing
for adverse shift. It is, we agree, a silly trope not intended to be taken
too seriously. Second, while we undoubtedly need better statistical
tests than those of equal distribution and of mean differences for testing
for adverse shift, we also happen to have a growing number of such
tests at our disposal. We should use them. Alright, this is long enough.
I need to get back to work on the next one [@kamulete2023bayes].
