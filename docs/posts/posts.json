[
  {
    "path": "posts/2023-01-03-are-you-ok/",
    "title": "Are you OK? Test for harmful (adverse) shift",
    "description": "Beyond statistical tests of equal distribution and mean difference for\ndataset shift.",
    "author": [
      {
        "name": "Vathy M. Kamulete",
        "url": "https://www.vathymut.org/"
      }
    ],
    "date": "2023-01-03",
    "categories": [],
    "contents": "\r\nSometimes the pertinent question is, are we worse off? Like in the\r\nsong, what we really want to know, Annie, is\r\nare you OK? We compare the way things were (the happy past) to the way things\r\nare (the tumultuous present). But wait, your statistical mind makes the leap,\r\nis this not just a two-sample comparison (test)? It is. But the trouble is not\r\nall statistical tests are well-suited to this question. The main point of\r\nof this post is to argue that some widely used statistical tests may come\r\nclose but often miss the mark when it comes to answering this. The\r\nQuestion ¬© ‚Äì once more so you don‚Äôt forget, are we worse off? ‚Äì is\r\nwhat I mean when I say testing for harmful or adverse shift.\r\n\r\nThe more covert point, the sinister one, the less altruistic one to put it\r\nkindly, is to motivate my own academic research, of course.\r\nBut first off, you may wonder, why does it matter? One reason to care in machine\r\nlearning is dataset shift. Over time, things change, data drift and predictive\r\nmodels suffer as a consequence: performance deteriorate, sometimes quite\r\ndrastically. Chip Huyen has a great primer on dataset shift\r\nhere\r\n- it is a fantastic resource (go read it). It turns out, to detect dataset\r\nshift, the industry often relies on statistical tests. The most common ones are\r\ntests of equal distributions and tests of mean difference. And I posit, both\r\nlet you down in perhaps surprising ways when it comes to the Question ¬©\r\nWhich is? You still with up, right?\r\nIn a previous\r\npost,\r\nI praised modern test of equal distribution. But I did warn you that I had\r\nsome beef too. It is time to settle the score. These tests fail us (it isn‚Äôt\r\ntheir fault; it‚Äôs ours, the users) when testing for harmful or adverse shifts\r\nbecause ‚Äúnot all changes in distribution are a cause for concern - some\r\nchanges are benign‚Äù (Kamulete 2022). Here is a simple pathological case\r\nto convince yourself of this. Suppose that your original sample was\r\ncontaminated with a few outliers. But Heaven smiles upon you; your current\r\ndata is free of outliers. You‚Äôre clearly better off. Well, tests of equal\r\ndistributions would still tell you that the two samples are different i.e.\r\nyou would reject the null of no difference. This is a false alarm. These\r\ntests don‚Äôt answer the Question ¬©; they tell us when both\r\nsamples can be said to be roughly equivalent, not whether we are worse off.\r\nAs we know all too well, different does not mean inferior. We need a better\r\ntest.\r\nLet‚Äôs now address the problem with statistical tests for\r\nmean differences. Let me pass it to Lindon, Sanden, and Shirikian (2022) who explains the problem\r\nquite succinctly:\r\n\r\n[..] we argue that performing inference about means is too limited for\r\ncanary tests, for not all bugs or performance regressions can be captured by\r\ndifferences in the mean alone [‚Ä¶]. Consider PlayDelay, an important key\r\nindicator of the streaming quality at Netflix, which measures the time taken\r\nfor a title to start once the user has hit the play button. It is possible for\r\nthe mean [‚Ä¶] to be the same, but for the tails of the\r\ndistribution to be heavier [‚Ä¶], resulting in an increased\r\noccurrence of extreme values. Large values of PlayDelay, even if infrequent,\r\nare unacceptable and considered a severe performance regression by our\r\nengineering teams.\r\n\r\nAgain, we need a better test. Incidentally, if you were not already convinced\r\nthat the Question ¬© is important, I know you are now: it is your Netflix\r\nbinge sessions at stake here. In short, both tests of\r\nequal distributions and of mean differences are lacking when it comes to the\r\nQuestion ¬©.\r\n\r\nIf you are a R user (get it? never mind), the\r\ndsos package\r\nimplements the approach in Kamulete (2022).\r\nThat‚Äôs the bad news. What‚Äôs the good news? If these widely used statistical\r\ntests are not good enough, what are the alternatives? I referred to two already\r\n(Kamulete 2022; Lindon, Sanden, and Shirikian 2022). Full disclosure: I am the author of\r\nKamulete (2022), published last year in UAI2022. Here are a few more that are\r\nanswering the Question ¬© in some ways more rigorously, more precisely\r\n(Podkopaev and Ramdas 2021; Luo et al. 2022; Vovk et al. 2021; Harel et al. 2014)\r\n1.\r\nWhen discussing my own work and the dsos package at\r\nuseR! 2022, I chose the title\r\n‚ÄúCall me when it hurts‚Äù because it tries to capture the essence of the\r\nQuestion ¬©. Data scientists want to know when there is a real problem:\r\nraising too many false alarms is one of the fastest and surest way to lose\r\ncredibility with your stakeholders.\r\n\r\nBy the way, the Safe, Anytime-Valid Inference\r\n(SAVI)\r\n2022 workshop (conference) is a great place to start if you‚Äôre looking\r\nfor more references.\r\nIf you stuck with me thus far, first I am sorry ‚Äì not\r\nsorry üòâ ‚Äì for abusing the Question ¬© as a short form for\r\ntesting for harmful shift. It is, we agree, a silly trope not meant to be\r\ntaken seriously. Let me conclude. While we undoubtedly need better statistical\r\ntests than those of equal distributions and of means difference for testing\r\nfor harmful or adverse shift, we also happen to have more and more\r\nsuch tests at our disposal. We should use them. Alright, this is long enough.\r\nI need to get back to work on the next one (Kamulete 2023).\r\n\r\n\r\n\r\nFlores, Miguel, Rub√©n Fern√°ndez-Casal, Salvador Naya, and Javier Tarrƒ±ÃÅo-Saavedra. 2021. ‚ÄúStatistical Quality Control with the Qcr Package.‚Äù R Journal 13 (1): 194‚Äì217.\r\n\r\n\r\nGandy, Axel, and Jan Terje Kval√∏y. 2017. ‚ÄúSpcadjust: An r Package for Adjusting for Estimation Error in Control Charts.‚Äù R J. 9 (1): 458.\r\n\r\n\r\nHarel, Maayan, Shie Mannor, Ran El-Yaniv, and Koby Crammer. 2014. ‚ÄúConcept Drift Detection Through Resampling.‚Äù In International Conference on Machine Learning, 1009‚Äì17. PMLR.\r\n\r\n\r\nKamulete, Vathy M. 2022. ‚ÄúTest for Non-Negligible Adverse Shifts.‚Äù In Uncertainty in Artificial Intelligence, 959‚Äì68. PMLR.\r\n\r\n\r\n‚Äî‚Äî‚Äî. 2023. ‚ÄúAre You OK? A Bayesian Sequential Test for Adverse Shift.‚Äù\r\n\r\n\r\nLindon, Michael, Chris Sanden, and Vach√© Shirikian. 2022. ‚ÄúRapid Regression Detection in Software Deployments Through Sequential Testing.‚Äù arXiv Preprint arXiv:2205.14762.\r\n\r\n\r\nLuo, Rachel, Rohan Sinha, Ali Hindy, Shengjia Zhao, Silvio Savarese, Edward Schmerling, and Marco Pavone. 2022. ‚ÄúOnline Distribution Shift Detection via Recency Prediction.‚Äù arXiv Preprint arXiv:2211.09916.\r\n\r\n\r\nPodkopaev, Aleksandr, and Aaditya Ramdas. 2021. ‚ÄúTracking the Risk of a Deployed Model and Detecting Harmful Distribution Shifts.‚Äù In International Conference on Learning Representations.\r\n\r\n\r\nVovk, Vladimir, Ivan Petej, Ilia Nouretdinov, Ernst Ahlberg, Lars Carlsson, and Alex Gammerman. 2021. ‚ÄúRetrain or Not Retrain: Conformal Test Martingales for Change-Point Detection.‚Äù In Conformal and Probabilistic Prediction and Applications, 191‚Äì210. PMLR.\r\n\r\n\r\nOne rich, active and effective paradigm that in my mind does not get its due\r\nin ML/AI circles is statistical process control or statistical quality\r\ncontrol (Gandy and Kval√∏y 2017; Flores et al. 2021). I am, unhappily I\r\nassure you, complicit in that here. This community also cares about the\r\nthe Question ¬©. In fact, some may rightfully say they were pioneers in\r\nthis.‚Ü©Ô∏é\r\n",
    "preview": {},
    "last_modified": "2023-01-03T23:25:03-05:00",
    "input_file": "are-you-ok.knit.md"
  },
  {
    "path": "posts/2022-01-22-in-gentle-praise-of-modern-tests/",
    "title": "In gentle praise of classifier tests",
    "description": "Two-sample statistical tests based on modern high-capacity\nclassifiers are powerful and still, underappreciated.",
    "author": [
      {
        "name": "Vathy M. Kamulete",
        "url": "https://www.vathymut.org/"
      }
    ],
    "date": "2022-01-22",
    "categories": [],
    "contents": "\r\nThe late great statistician, Sir David Cox,\r\nhas a talk titled In gentle praise of significance tests.\r\nIt is, of course, only gentle praise because inevitably, there are some\r\nissues. I too have a bone to pick. But\r\nwe will get to the beef much later. First, the praise.\r\nRemember the old two-sample tests? They go by different names: test of equal\r\ndistribution, of homogeneity, of goodness-of-fit, etc. The gist of it is you\r\nhave two samples. And so, you wonder, are they similar enough? Rings a bell,\r\nright? The Kolmogorov‚ÄìSmirnov\r\ntest is a\r\nclassic for this sort of thing. But the\r\nK-S test is old news, well before the\r\nadvent of big and high dimensional data üò±.\r\nSince then, the field has been churning out powerful two-sample tests for the\r\nBig Data era. Every year, exciting works are published on the subject. I\r\nrecently messed about with the Ball divergence approach (Pan et al. 2018).\r\nYou, no doubt, chose your own adventure. There has been tremendous\r\nprogress. Let Uncle Larry\r\nexplain1.\r\nAnd now you are, more or less, caught up with the state-of-the-art in\r\ntwo-sample tests. The End.\r\n\r\nThat is, err Larry A.\r\nWasserman.\r\nOK, not so fast. You see, Uncle Larry left out\r\nsomething important. He touches on 3 ideas: kernel, energy, and cross-match\r\ntests. Trust me when I say, he wants you to know about the classifier tests too\r\n(Kim et al. 2021). Here is how they work. You assign your samples to\r\ndifferent classes (you give them labels). Sample 1 is the positive class,\r\nand Sample 2, the negative one or vice versa. Then, you fit your\r\nfavorite classifier to see if it can reliably predict the label. If\r\nit can, it means the two samples are probably different. If\r\nit cannot, the two samples are similar enough. It seems obvious in hindsight,\r\ndoes it not? It is not even deceptively simple: it is actually simple.\r\n\r\nDetails, casually swept under the rug, include deriving \\(p-\\)value from the\r\nclassifier performance. The short answer is permutation is your friend.\r\nAnd yet, most people do not seem to know about two-sample classifier tests.\r\nIn comparison, kernel tests seem ubiquitous (if you are looking at academic\r\njournals, not if you‚Äôre hanging out on Twitter). Even the energy tests, one of the\r\ninnovation that Uncle Larry discusses, are in a fundamental sense equivalent to\r\nthe kernel tests (Sejdinovic et al. 2012; Shen and Vogelstein 2020). It is kernels all\r\nthe way down. On the theoretical front, the mathematical maturity\r\nto wield this awesome kernel power may require arcane incantations to\r\nRKHS voodoo magic.\r\nBut no matter, word to Eric B. & Rakim, we won‚Äôt\r\nsweat the technique.\r\nStill, this all begs the question. How do the humble classifier tests stack up\r\nagainst the more celebrated kernel tests? No spoiler from me. Let\r\nLopez-Paz and Oquab (2017) do it:\r\n\r\nOur take-home message is that modern binary classifiers can be easily turned\r\ninto powerful two-sample tests. We have shown that these classifier two-sample\r\ntests set a new state-of-the-art in performance, and enjoy unique attractive\r\nproperties: they are easy to implement, learn a representation of the data\r\non the fly, have simple asymptotic distributions, and allow different ways to\r\ninterpret how the two samples under study differ.\r\n\r\nThey do not often tell you this. You can in fact\r\nthink of a kernel test as a classifier test.\r\nAnd you know what, data scientists are really good at supervised\r\nclassification. Say \\(p-\\)values, eyes glaze\r\nover and no one is listening. Say 50% accuracy instead, suddenly everyone\r\nunderstands and is nodding vigorously. No RKHS magic needed ‚Äì this is yuge.\r\n\r\n\r\n\r\nFrom GIPHY\r\n\r\nHere is Cai, Goggin, and Jiang (2020), more recently, to hammer the point home:\r\n\r\nWe propose a test for two-sample problem based on estimates of classification\r\nprobabilities obtained from a consistent classification algorithm. [‚Ä¶] Our\r\ntest is more powerful and efficient than many other tests.\r\n\r\nPracticing data scientists cannot afford to ignore two-sample classifier tests.\r\nThey are powerful, easy to implement and easy to explain, the elusive\r\ntrifecta2. Plenty of works advocate for the\r\nclassifier tests (Friedman 2004; Vayatis, Depecker, and Cl√©men√ßcon 2009; Liu, Li, and P√≥czos 2018; Hediger, Michel, and N√§f 2019). Still,\r\nthey remain for the most part underappreciated. No one\r\n(anyone you know?) brags about them, the way for example that they would about\r\nthe newest kernel test on the block. I suspect it is because the theory seems\r\nboring in comparison. In practice, these guys pack a punch. This is worth\r\npraising. The two-sample classifier tests need more love. They deserve it.\r\n\r\n\r\n\r\nCai, Haiyan, Bryan Goggin, and Qingtang Jiang. 2020. ‚ÄúTwo-Sample Test Based on Classification Probability.‚Äù Statistical Analysis and Data Mining: The ASA Data Science Journal 13 (1): 5‚Äì13.\r\n\r\n\r\nFriedman, Jerome. 2004. ‚ÄúOn Multivariate Goodness-of-Fit and Two-Sample Testing.‚Äù Stanford Linear Accelerator Center, Menlo Park, CA (US).\r\n\r\n\r\nHediger, Simon, Loris Michel, and Jeffrey N√§f. 2019. ‚ÄúOn the Use of Random Forest for Two-Sample Testing.‚Äù arXiv Preprint arXiv:1903.06287.\r\n\r\n\r\nKim, Ilmun, Aaditya Ramdas, Aarti Singh, and Larry Wasserman. 2021. ‚ÄúClassification Accuracy as a Proxy for Two-Sample Testing.‚Äù The Annals of Statistics 49 (1): 411‚Äì34.\r\n\r\n\r\nLiu, Yusha, Chun-Liang Li, and Barnab√°s P√≥czos. 2018. ‚ÄúClassifier Two Sample Test for Video Anomaly Detections.‚Äù In BMVC, 71.\r\n\r\n\r\nLopez-Paz, David, and Maxime Oquab. 2017. ‚ÄúRevisiting Classifier Two-Sample Tests.‚Äù In International Conference on Learning Representations.\r\n\r\n\r\nPan, Wenliang, Yuan Tian, Xueqin Wang, and Heping Zhang. 2018. ‚ÄúBall Divergence: Nonparametric Two Sample Test.‚Äù Annals of Statistics 46 (3): 1109.\r\n\r\n\r\nSejdinovic, Dino, Arthur Gretton, Bharath Sriperumbudur, and Kenji Fukumizu. 2012. ‚ÄúHypothesis Testing Using Pairwise Distances and Associated Kernels.‚Äù In 29th International Conference on Machine Learning, ICML 2012, 1111‚Äì18.\r\n\r\n\r\nShen, Cencheng, and Joshua T Vogelstein. 2020. ‚ÄúThe Exact Equivalence of Distance and Kernel Methods in Hypothesis Testing.‚Äù AStA Advances in Statistical Analysis, 1‚Äì19.\r\n\r\n\r\nVayatis, Nicolas, Marine Depecker, and St√©phan Cl√©men√ßcon. 2009. ‚ÄúAUC Optimization and the Two-Sample Problem.‚Äù Advances in Neural Information Processing Systems 22: 360‚Äì68.\r\n\r\n\r\nClick\r\non that link. I won‚Äôt be offended; I will in fact, happily, wait.‚Ü©Ô∏é\r\nThink the son, the father and the holy spirit, the divine\r\ntrinity.‚Ü©Ô∏é\r\n",
    "preview": {},
    "last_modified": "2023-01-03T02:28:24-05:00",
    "input_file": {}
  }
]
