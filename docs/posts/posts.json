[
  {
    "path": "posts/2022-01-22-in-gentle-praise-of-modern-tests/",
    "title": "In gentle praise of classifiers as tests",
    "description": "Modern two-sample statistical tests based on today's high-capacity\nclassifiers are powerful and still, underappreciated.",
    "author": [
      {
        "name": "Vathy M. Kamulete",
        "url": "https://www.vathymut.org/"
      }
    ],
    "date": "2022-01-22",
    "categories": [],
    "contents": "\r\nThe late great statistician, Sir David Cox, has a talk titled In gentle praise of significance tests. It is, of course, only gentle praise because inevitably, there are some issues to deal with. I too have a bone to pick. But we will get to the beef much later. First, the praise.\r\nRemember the old two-sample tests? They go by different names: test of equal distribution, of homogeneity, of goodness-of-fit, etc. The gist of it is you have two samples. And so, you wonder, are they the same? Are the two samples drawn from the same population? Rings a bell, right? The Kolmogorov‚ÄìSmirnov test is a classic and is typically one of the first statistical test you learn for this. But the K-S test is for yesteryear, before the advent of big and high dimensional data üò±.\r\nSince then, the field has come up with powerful two-sample tests for the Big Data era. Every year, exciting works are published on the subject. I recently messed about with the Ball divergence approach (Pan et al. 2018). You, no doubt, have your chosen own adventure. There has been tremendous progress. Let Uncle Larry explain.1\r\n\r\nThat is, err Larry A. Wasserman.\r\nAnd now you are, more or less, caught up with the state-of-the-art on two-sample tests. The End. OK, not so fast. You see, Uncle Larry left out something important. He discusses 3 ideas: kernel, energy, and cross-match tests. Trust me when I say, he wants you to know about the classifier tests too (Kim et al. 2021). Here is how they work. You assign your samples to different classes (you give them labels). Sample 1 is the positive class, and Sample 2, the negative one or vice versa. Then, you fit your favorite classifier to see if it can reliably predict sample membership. If it can with high accuracy, it means the two samples are probably different. If it cannot, the two samples are similar enough. To use the jargon, the null hypothesis holds.\r\n\r\nDetails, casually swept under the rug, include deriving \\(p-\\)value from the classifier performance. The short answer is permutation is your friend.\r\nThe thing is most people do not seem to know about two-sample classifier tests. In comparison, kernel tests seem ubiquitous (if you are looking at academic journals, not if you‚Äôre hanging out on Twitter). Even energy (distance-based) tests are in some fundamental sense also kernel tests (Sejdinovic et al. 2012; Shen and Vogelstein 2020). Do not get me wrong. I like me some kernels too. The math is deep and beautiful ‚Äì you get arcane incantations to voodoo magic such as RKHS theory. But it begs the question. How do classifier tests stack up against kernel tests?\r\nNo spoiler from me. Let Lopez-Paz and Oquab (2017) do it:\r\n\r\nOur take-home message is that modern binary classifiers can be easily turned into powerful two-sample tests. We have shown that these classifier two-sample tests set a new state-of-the-art in performance, and enjoy unique attractive properties: they are easy to implement, learn a representation of the data on the fly, have simple asymptotic distributions, and allow different ways to interpret how the two samples under study differ.\r\n\r\nThey do not often tell you this. You can in fact think of a kernel test as a classifier test. And you know what, data scientists are really good at supervised classification. It is our bread and butter; it is Kaggle 101. Talk statistical tests and \\(p-\\)values, eyes glaze over and no one is listening. Say 50% accuracy instead, suddenly everyone understands and is nodding vigorously. No RKHS magic needed ‚Äì this is yuge.\r\n\r\n\r\n\r\nFrom GIPHY\r\n\r\nHere is Cai, Goggin, and Jiang (2020), more recently, to hammer the point home:\r\n\r\nWe propose a test for two-sample problem based on estimates of classification probabilities obtained from a consistent classification algorithm. [‚Ä¶] Our test is more powerful and efficient than many other tests.\r\n\r\nPracticing data scientists cannot afford to ignore two-sample classifier tests. They are powerful, easy to implement and easy to explain, the elusive trifecta2. Plenty of works advocate for the two-sample classifier test (Friedman 2004; Vayatis, Depecker, and Cl√©men√ßcon 2009; Liu, Li, and P√≥czos 2018; Hediger, Michel, and N√§f 2019; Kirchler et al. 2020). Still, they remain for the most part underappreciated. No one (anyone you know?) brags about them, the way for example that they would about the newest kernel test on the block. I suspect it is because the theory seems boring in comparison. In practice, these guys pack a punch. This is worth praising. The two-sample classifier tests need more love. They deserve it.\r\n\r\n\r\n\r\nCai, Haiyan, Bryan Goggin, and Qingtang Jiang. 2020. ‚ÄúTwo-Sample Test Based on Classification Probability.‚Äù Statistical Analysis and Data Mining: The ASA Data Science Journal 13 (1): 5‚Äì13.\r\n\r\n\r\nFriedman, Jerome. 2004. ‚ÄúOn Multivariate Goodness-of-Fit and Two-Sample Testing.‚Äù Stanford Linear Accelerator Center, Menlo Park, CA (US).\r\n\r\n\r\nHediger, Simon, Loris Michel, and Jeffrey N√§f. 2019. ‚ÄúOn the Use of Random Forest for Two-Sample Testing.‚Äù arXiv Preprint arXiv:1903.06287.\r\n\r\n\r\nKim, Ilmun, Aaditya Ramdas, Aarti Singh, and Larry Wasserman. 2021. ‚ÄúClassification Accuracy as a Proxy for Two-Sample Testing.‚Äù The Annals of Statistics 49 (1): 411‚Äì34.\r\n\r\n\r\nKirchler, Matthias, Shahryar Khorasani, Marius Kloft, and Christoph Lippert. 2020. ‚ÄúTwo-Sample Testing Using Deep Learning.‚Äù In International Conference on Artificial Intelligence and Statistics, 1387‚Äì98. PMLR.\r\n\r\n\r\nLiu, Yusha, Chun-Liang Li, and Barnab√°s P√≥czos. 2018. ‚ÄúClassifier Two Sample Test for Video Anomaly Detections.‚Äù In BMVC, 71.\r\n\r\n\r\nLopez-Paz, David, and Maxime Oquab. 2017. ‚ÄúRevisiting Classifier Two-Sample Tests.‚Äù In International Conference on Learning Representations.\r\n\r\n\r\nPan, Wenliang, Yuan Tian, Xueqin Wang, and Heping Zhang. 2018. ‚ÄúBall Divergence: Nonparametric Two Sample Test.‚Äù Annals of Statistics 46 (3): 1109.\r\n\r\n\r\nSejdinovic, Dino, Arthur Gretton, Bharath Sriperumbudur, and Kenji Fukumizu. 2012. ‚ÄúHypothesis Testing Using Pairwise Distances and Associated Kernels.‚Äù In 29th International Conference on Machine Learning, ICML 2012, 1111‚Äì18.\r\n\r\n\r\nShen, Cencheng, and Joshua T Vogelstein. 2020. ‚ÄúThe Exact Equivalence of Distance and Kernel Methods in Hypothesis Testing.‚Äù AStA Advances in Statistical Analysis, 1‚Äì19.\r\n\r\n\r\nVayatis, Nicolas, Marine Depecker, and St√©phan Cl√©men√ßcon. 2009. ‚ÄúAUC Optimization and the Two-Sample Problem.‚Äù Advances in Neural Information Processing Systems 22: 360‚Äì68.\r\n\r\n\r\nClick on that link. I won‚Äôt be offended; I will in fact, happily, wait.‚Ü©Ô∏é\r\nThink the son, the father and the holy spirit, the divine trinity, meaning no offense. This being the internet, I am sure I have anyway.‚Ü©Ô∏é\r\n",
    "preview": {},
    "last_modified": "2022-01-25T12:39:14-05:00",
    "input_file": {}
  }
]
